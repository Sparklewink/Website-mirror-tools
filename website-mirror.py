import os
import re
import shutil
from urllib.parse import urljoin, urlparse, unquote
from urllib import robotparser  # æ–°å¢
from pathlib import Path
import http.server
import socketserver
import threading
import time

import requests
from bs4 import BeautifulSoup

# --- é…ç½® ---
TARGET_URL = "http://example.com"  # è¯·ä¿®æ”¹ä¸ºä½ æƒ³è¦é•œåƒçš„ç½‘ç«™URL
DOWNLOAD_DIR = "mirrored_site" #ä¸‹è½½æ–‡ä»¶å¤¹
SERVER_PORT = 712 #ç«¯å£å·ï¼Œå¯è‡ªå·±ä¿®æ”¹
RESPECT_ROBOTS_TXT = False  # True: éµå®ˆrobots.txt, False: å¿½ç•¥robots.txt

# å…¨å±€å˜é‡
urls_to_visit = set()
visited_urls = set()
session = requests.Session()
session.headers.update({
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 MirrorBot/1.0"
})

robot_parsers = {}  # å­˜å‚¨ RobotFileParser å®ä¾‹ {domain: parser}
last_request_times = {}  # å­˜å‚¨æ¯ä¸ªåŸŸåçš„æœ€åè¯·æ±‚æ—¶é—´ {domain: timestamp}


class AllowAllRobotParserPlaceholder:
    """ä¸€ä¸ªåœ¨robots.txtè¯»å–å¤±è´¥æ—¶ä½¿ç”¨çš„å ä½è§£æå™¨ï¼Œå…è®¸æ‰€æœ‰æ“ä½œã€‚"""

    def can_fetch(self, useragent, url):
        return True

    def crawl_delay(self, useragent):
        return None

    def request_rate(self, useragent):  # (requests, seconds)
        return None


def get_robot_parser_for_url(url_str):
    """è·å–æˆ–åˆ›å»ºã€è¯»å–å¹¶ç¼“å­˜æŒ‡å®šURLåŸŸåçš„RobotFileParserã€‚"""
    parsed_url = urlparse(url_str)
    domain = parsed_url.netloc
    robots_url = f"{parsed_url.scheme}://{domain}/robots.txt"

    if domain not in robot_parsers:
        print(f"    [i] æ­£åœ¨ä¸º {domain} è¯»å– robots.txt")
        rp = robotparser.RobotFileParser()
        rp.set_url(robots_url)
        try:
            rp.read()
        except Exception as e:
            print(f"    [!] è¯»å– {domain} çš„ robots.txt å¤±è´¥: {e}. å‡è®¾å…è®¸æ‰€æœ‰ã€‚")
            rp = AllowAllRobotParserPlaceholder()
        robot_parsers[domain] = rp
    return robot_parsers[domain]


def sanitize_path(path_segment):
    """æ¸…ç†è·¯å¾„æ®µï¼Œç§»é™¤ä¸å®‰å…¨å­—ç¬¦ï¼Œå¹¶å°†æŸ¥è¯¢å‚æ•°ç­‰ä½œä¸ºæ–‡ä»¶åçš„ä¸€éƒ¨åˆ†ï¼ˆç®€åŒ–å¤„ç†ï¼‰"""
    decoded_segment = unquote(path_segment)
    sanitized = re.sub(r'[<>:"/\\|?*]', '_', decoded_segment)
    sanitized = sanitized.strip('. ')
    if not sanitized:
        return "_"
    return sanitized


def get_local_path(base_url_netloc, current_url_str, download_dir_base):
    """æ ¹æ®URLç”Ÿæˆæœ¬åœ°æ–‡ä»¶è·¯å¾„"""
    parsed_url = urlparse(current_url_str)
    path_parts = [sanitize_path(part) for part in parsed_url.path.strip('/').split('/') if part]

    filename = "index.html"
    if path_parts:
        if '.' in path_parts[-1] or len(path_parts[-1]) > 50:  # Heuristic for filename-like part
            filename = path_parts.pop()

    if parsed_url.query:
        filename += "_" + sanitize_path(parsed_url.query)
    if parsed_url.fragment:
        filename += "_" + sanitize_path(parsed_url.fragment)

    if not filename and not path_parts:
        filename = "index.html"
    elif not filename and path_parts:
        filename = "index.html"

    local_dir = Path(download_dir_base)
    for part in path_parts:
        local_dir = local_dir / part

    return local_dir / filename, local_dir


def download_and_process_url(url_to_fetch, base_domain, download_root):
    """ä¸‹è½½URLå†…å®¹ï¼Œå¦‚æœæ˜¯HTMLåˆ™è§£æå¹¶æŸ¥æ‰¾æ›´å¤šé“¾æ¥ã€‚è¿”å›Trueè¡¨ç¤ºæˆåŠŸï¼ŒFalseè¡¨ç¤ºå¤±è´¥ã€‚"""
    print(f"[*] æ­£åœ¨è®¿é—®: {url_to_fetch}")

    try:
        response = session.get(url_to_fetch, timeout=15, stream=True)  # Increased timeout
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"[!] è¯·æ±‚å¤±è´¥ {url_to_fetch}: {e}")
        visited_urls.add(url_to_fetch)  # æ ‡è®°ä¸ºå·²è®¿é—®ï¼Œå³ä½¿å¤±è´¥ï¼Œä»¥é¿å…é‡è¯•
        return False

    content_type = response.headers.get('content-type', '').lower()
    local_filepath, local_dir = get_local_path(base_domain, url_to_fetch, download_root)

    local_dir.mkdir(parents=True, exist_ok=True)

    with open(local_filepath, 'wb') as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)
    print(f"    -> å·²ä¿å­˜åˆ°: {local_filepath}")
    visited_urls.add(url_to_fetch)  # æ ‡è®°ä¸ºå·²è®¿é—®ï¼ˆæˆåŠŸä¸‹è½½åï¼‰

    if 'text/html' in content_type:
        encoding = response.encoding or 'utf-8'
        try:
            with open(local_filepath, 'r', encoding=encoding, errors='replace') as f_read:
                soup = BeautifulSoup(f_read.read(), 'html.parser')
        except Exception as e:
            print(f"[!] è§£æHTMLå¤±è´¥ {local_filepath}: {e}")
            return True  # æ–‡ä»¶å·²ä¸‹è½½ï¼Œä½†è§£æå¤±è´¥

        modified = False
        for tag_name, attr_name in [('a', 'href'), ('link', 'href'),
                                    ('img', 'src'), ('script', 'src')]:
            for tag in soup.find_all(tag_name):
                attr_value = tag.get(attr_name)
                if not attr_value:
                    continue

                absolute_url = urljoin(url_to_fetch, attr_value)
                parsed_absolute_url = urlparse(absolute_url)

                if parsed_absolute_url.netloc == base_domain and \
                        parsed_absolute_url.scheme in ['http', 'https']:

                    target_local_path_obj, _ = get_local_path(base_domain, absolute_url, download_root)
                    current_file_dir_obj = local_filepath.parent

                    try:
                        relative_new_path = os.path.relpath(target_local_path_obj, current_file_dir_obj)
                        relative_new_path = relative_new_path.replace(os.sep, '/')

                        if tag[attr_name] != relative_new_path:
                            # print(f"    ğŸ”— æ›´æ–°é“¾æ¥: {tag[attr_name]} -> {relative_new_path}")
                            tag[attr_name] = relative_new_path
                            modified = True
                    except ValueError as ve:
                        print(f"[!] æ— æ³•è®¡ç®—ç›¸å¯¹è·¯å¾„: {target_local_path_obj} from {current_file_dir_obj} - {ve}")

                    if absolute_url not in visited_urls and absolute_url not in urls_to_visit:
                        urls_to_visit.add(absolute_url)

        if modified:
            try:
                with open(local_filepath, 'w', encoding=encoding, errors='replace') as f_write:
                    f_write.write(str(soup))
                print(f"    -> å·²æ›´æ–°é“¾æ¥å¹¶ä¿å­˜: {local_filepath}")
            except Exception as e:
                print(f"[!] å†™å…¥ä¿®æ”¹åçš„HTMLå¤±è´¥ {local_filepath}: {e}")
    return True


def start_mirroring(target_url_str, download_dir_str):
    """ä¸»é•œåƒé€»è¾‘"""
    download_root_path = Path(download_dir_str)
    if download_root_path.exists():
        print(f"[*] æ¸…ç†æ—§çš„é•œåƒç›®å½•: {download_dir_str}")
        shutil.rmtree(download_root_path)
    download_root_path.mkdir(parents=True, exist_ok=True)

    parsed_target_url = urlparse(target_url_str)
    base_domain = parsed_target_url.netloc
    user_agent_for_robots = session.headers.get("User-Agent", "*")

    if not base_domain:
        print(f"[!] æ— æ•ˆçš„ç›®æ ‡URLï¼Œæ— æ³•æå–åŸŸå: {target_url_str}")
        return

    urls_to_visit.add(target_url_str)

    while urls_to_visit:
        current_url = urls_to_visit.pop()

        if current_url in visited_urls:
            continue

        current_domain = urlparse(current_url).netloc  # Should always be base_domain

        if RESPECT_ROBOTS_TXT:
            rp = get_robot_parser_for_url(current_url)

            if not rp.can_fetch(user_agent_for_robots, current_url):
                print(f"[*] è·³è¿‡ (robots.txtç¦æ­¢): {current_url}")
                visited_urls.add(current_url)  # æ ‡è®°ä¸ºå·²è®¿é—®ï¼Œé¿å…é‡è¯•
                continue

            delay = rp.crawl_delay(user_agent_for_robots)
            if delay:
                last_request_time = last_request_times.get(current_domain, 0)
                wait_time = (last_request_time + delay) - time.time()
                if wait_time > 0:
                    print(f"    [i] éµå®ˆCrawl-delay: ç­‰å¾… {wait_time:.2f}s ({current_domain})")
                    time.sleep(wait_time)

        download_successful = download_and_process_url(current_url, base_domain, download_root_path.resolve())

        if RESPECT_ROBOTS_TXT and download_successful:  # è®°å½•æˆåŠŸè¯·æ±‚çš„æ—¶é—´
            last_request_times[current_domain] = time.time()

        # å¯é€‰ï¼šå³ä½¿ä¸éµå®ˆrobots.txtï¼Œä¹ŸåŠ ä¸€ä¸ªå°å»¶æ—¶
        # if not (RESPECT_ROBOTS_TXT and delay): # å¦‚æœæ²¡æœ‰ crawl-delay æ§åˆ¶
        #     time.sleep(0.1) # é€šç”¨å°å»¶æ—¶

    print("\n[*] é•œåƒè¿‡ç¨‹å®Œæˆ!")
    print(f"[*] æ–‡ä»¶ä¿å­˜åœ¨: {download_root_path.resolve()}")


def run_server(port, directory):
    """å¯åŠ¨ä¸€ä¸ªç®€å•çš„HTTPæœåŠ¡å™¨æ¥æŸ¥çœ‹é•œåƒæ•ˆæœ"""

    class Handler(http.server.SimpleHTTPRequestHandler):
        def __init__(self, *args, **kwargs):
            super().__init__(*args, directory=str(directory), **kwargs)

    # ç¡®ä¿ç›®å½•å­˜åœ¨æ‰å¯åŠ¨æœåŠ¡å™¨
    if not Path(directory).exists():
        print(f"[!] é”™è¯¯ï¼šé•œåƒç›®å½• '{directory}' ä¸å­˜åœ¨ï¼Œæ— æ³•å¯åŠ¨æœåŠ¡å™¨ã€‚")
        return

    with socketserver.TCPServer(("", port), Handler) as httpd:
        print(f"\n[*] åœ¨ http://localhost:{port} å¯åŠ¨æœ¬åœ°æœåŠ¡å™¨")
        print(f"[*] æœåŠ¡ç›®å½•: {directory}")
        print("[*] æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨ã€‚")
        try:
            httpd.serve_forever()
        except KeyboardInterrupt:
            print("\n[*] æœåŠ¡å™¨å·²åœæ­¢ã€‚")
            httpd.shutdown()
        except OSError as e:
            print(f"\n[!] æœåŠ¡å™¨å¯åŠ¨å¤±è´¥ (ç«¯å£ {port} å¯èƒ½å·²è¢«å ç”¨): {e}")


if __name__ == "__main__":
    if not TARGET_URL or TARGET_URL == "http://example.com":  # æé†’ç”¨æˆ·ä¿®æ”¹
        print("[-] è¯·ç¼–è¾‘è„šæœ¬ï¼Œä¿®æ”¹ TARGET_URL ä¸ºä½ æƒ³è¦é•œåƒçš„ç½‘ç«™ã€‚")
        print("[-] ä½ ä¹Ÿå¯ä»¥ä¿®æ”¹ RESPECT_ROBOTS_TXT (å½“å‰ä¸º {}) æ¥æ§åˆ¶æ˜¯å¦éµå®ˆrobots.txtã€‚".format(RESPECT_ROBOTS_TXT))
    else:
        start_mirroring(TARGET_URL, DOWNLOAD_DIR)

        # ç¡®ä¿DOWNLOAD_DIRæ˜¯Pathå¯¹è±¡ä»¥ä¾¿resolve()
        # å¹¶ä¸”åœ¨å¯åŠ¨æœåŠ¡å™¨å‰æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
        server_dir = Path(DOWNLOAD_DIR).resolve()
        if server_dir.exists():
            server_thread = threading.Thread(target=run_server, args=(SERVER_PORT, server_dir))
            server_thread.daemon = True
            server_thread.start()

            try:
                while server_thread.is_alive():
                    server_thread.join(timeout=1)
            except KeyboardInterrupt:
                print("\n[*] æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨å…³é—­...")
        else:
            print(f"\n[!] é•œåƒç›®å½• '{DOWNLOAD_DIR}' æœªåˆ›å»ºæˆ–ä¸ºç©ºï¼ŒæœåŠ¡å™¨æœªå¯åŠ¨ã€‚")